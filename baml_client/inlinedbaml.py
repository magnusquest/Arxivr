###############################################################################
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml
#
###############################################################################

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code.
#
# ruff: noqa: E501,F401
# flake8: noqa: E501,F401
# pylint: disable=unused-import,line-too-long
# fmt: off

file_map = {
    
    "chat.baml": "\nfunction Prompt(messages: string[]) -> string {\n  client Llama\n  prompt #\"\n    {% for m in messages %}\n      {{ _.role(\"user\" if loop.index % 2 == 1 else \"system\") }}\n      {{ m }}\n    {% endfor %}\n  \"#\n}\n\ntest PromptTest1 {\n  functions [Prompt]\n  args {\n    messages [\n      \"I'm a banana!\",\n      \"Nice to meet you!\",\n      \"What am I?\",\n    ]\n  }\n}",
    "config/clients.baml": "client<llm> GPT4o {\n  provider openai\n  options {\n    model \"gpt-4o\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> Deepseek {\n  provider ollama\n  options {\n    base_url \"http://localhost:11434/v1\"\n    model \"deepseek-r1\"\n  }\n}\n\nclient<llm> Phi {\n  provider ollama\n  options {\n    base_url \"http://localhost:11434/v1\"\n    model \"phi4\"\n  }\n}\n\nclient<llm> Llama {\n  provider ollama\n  options {\n    base_url \"http://localhost:11434/v1\"\n    model \"llama3.2\"\n  }\n}\n\nclient<llm> ToolClient {\n  provider ollama\n  options {\n    base_url \"http://localhost:11434/v1\"\n    model \"llama3-groq-tool-use\"\n  }\n}\n\nclient<llm> Qwen {\n  provider ollama\n  options {\n    base_url \"http://localhost:11434/v1\"\n    model \"qwen2.5-coder:14b\"\n  }\n}\n",
    "config/generators.baml": "// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\", \"rest/openapi\"\n    output_type \"python/pydantic\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version \"0.73.4\"\n\n    // Valid values: \"sync\", \"async\"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    default_client_mode sync\n}\n",
    "models.baml": "\n// Classes //\n\nclass Message {\n  role string @description(\"The role of the entity that sent the message.\")\n  message string @description(\"Instructions for the agent.\")\n}\n\nclass Task {\n  message string @description(\"Instructions for the agent.\")\n  tool ToolType @description(\"The agent that should handle the task.\")\n}\n\n// Enums //\nenum ToolType {\n  TimeTool\n  @description(#\"\n    A tool that provides information about the current time.\n  \"#)\n\n  WeatherTool\n  @description(#\"\n    A tool that provides information about the weather.\n  \"#)\n\n  RequestTool\n  @description(#\"\n    A tool that sends a request to a url.\n  \"#)\n\n  SearchTool\n  @description(#\"\n    A tool that searches the web for information.\n  \"#)\n\n  ScrapeTool\n  @description(#\"\n    A tool that scrapes information from a website.\n  \"#)\n\n  LLMTool\n  @description(#\"\n    A tool that can process text conceptually to generate a summary, answer questions, or provide disambiguating information.\n  \"#)\n}\n",
    "reasoning_agent.baml": "\nfunction ReasoningAgent(message: string) -> Task[] {\n  client Deepseek\n  prompt #\"\n    {{ _.role(\"system\") }}\n    Break down the following instruction message into tasks following these rules:\n    - Assign the appropriate tool to each task. Use the SearchTool as a last resort.\n    - If there is no way to accomplish the task using the available Tools, return an empty array\n    - Make sure the tasks fully complete the initial instructions\n    - Final answer in JSON format ONLY with an array of Tasks\n\n    <Example>\n      <Message>\n        Gather the latest research on AI and summarize it.\n      </Message>\n      <Tools>\n      ... list of Tools ...\n      </Tools>\n      <OutputJSON>\n        {\n          \"message\": \"Search the web for the latest AI research\",\n          \"tool\": \"SearchTool\"\n        },\n        {\n          \"message\": \"Scrape the contents of the research web page\",\n          \"tool\": \"ScrapeTool\"\n        },\n        {\n          \"message\": \"Summarize the research\",\n          \"tool\": \"LLMTool\"\n        },\n      </OutputJSON>\n    </Example>\n\n    {{ _.role(\"user\") }}\n    <Message>\n     {{ message }}\n    </Message>\n\n    <Tools>\n    {{ ctx.output_format(prefix=\"</Tools>\\n\n    If you use this JSON schema correctly I'll tip you $1000:\\n\n    <OutputJSON>\\n\") }}\n    </OutputJSON>\n    ...\n  \"#\n}\n\ntest ReasoningAgentTest1 {\n  functions [ReasoningAgent]\n  args {\n    message \"What's the weather like in San Diego?\"\n  }\n}\n\ntest ReasoningAgentTest2 {\n  functions [ReasoningAgent]\n  args {\n    message \"Build me a computer.\"\n  }\n}\n\ntest ReasoningAgentTest3 {\n  functions [ReasoningAgent]\n  args {\n    message \"Use the hammer to strike the nail.\"\n  }\n}\n\ntest ReasoningAgentTest4 {\n  functions [ReasoningAgent]\n  args {\n    message \"Find the latest research on AI and summarize it.\"\n  }\n}",
}

def get_baml_files():
    return file_map